import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json
import time

def NewsPars(output_file):
    section_urls = input(
        "–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–∞–∑–¥–µ–ª—ã AP News (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é): "
    ).strip().split(",")

    section_urls = [url.strip() for url in section_urls if url.strip()]
    max_articles_per_section = 500

    total_parsed = 0

    for section_url in section_urls:
        print(f"[INFO] –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∏–∑ —Ä–∞–∑–¥–µ–ª–∞: {section_url}")
        links = get_article_links(section_url, max_articles=max_articles_per_section)
        print(f"[INFO] –ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫")

        for i, url in enumerate(links, 1):
            article = parse_article(url, output_file=output_file)
            if article:
                total_parsed += 1

            if i % 10 == 0:
                print(f"[INFO] –°–ø–∞—Ä—Å–∏–ª–∏ {i} —Å—Ç–∞—Ç–µ–π –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞")

            time.sleep(3)

        print(f"[INFO] –†–∞–∑–¥–µ–ª –∑–∞–≤–µ—Ä—à—ë–Ω: {section_url}")

    print(f"[INFO] –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {total_parsed}")
    print("[INFO] –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω")


#def NewsPars():
#     # –í–≤–æ–¥ —Å—Ä–∞–∑—É –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
#     section_urls = input("–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–∞–∑–¥–µ–ª—ã AP News (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é): ").strip().split(",")
#     section_urls = [url.strip() for url in section_urls if url.strip()]
#     max_articles_per_section = 200  # –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å
#
#     all_articles = []
#
#     for section_url in section_urls:
#         print(f"[INFO] –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ä–∞–∑–¥–µ–ª–∞: {section_url}")
#         links = get_article_links(section_url, max_articles=max_articles_per_section)
#         print(f"[INFO] –ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫. –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥...")
#
#         for i, url in enumerate(links, 1):
#             article = parse_article(url)
#             if article:
#                 all_articles.append(article)
#             if i % 10 == 0:
#                 print(f"[INFO] –°–ø–∞—Ä—Å–∏–ª–∏ {i} —Å—Ç–∞—Ç–µ–π –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞...")
#             time.sleep(3)  # –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏, —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å 429
#
#         print(f"[INFO] –†–∞–∑–¥–µ–ª {section_url} –∑–∞–≤–µ—Ä—à–µ–Ω. –°–ø–∞—Ä—Å–∏–ª–∏ {len(links)} —Å—Ç–∞—Ç–µ–π.")
#
#     print(f"[INFO] –í—Å–µ–≥–æ —Å–ø–∞—Ä—Å–∏–ª–∏ {len(all_articles)} —Å—Ç–∞—Ç–µ–π —Å–æ –≤—Å–µ—Ö —Ä–∞–∑–¥–µ–ª–æ–≤.")
#
#     # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSONL
#     output_file = "apnews_articles.jsonl"
#     with open(output_file, "w", encoding="utf-8") as f:
#         for article in all_articles:
#             f.write(json.dumps(article, ensure_ascii=False) + "\n")
#
#     print(f"[INFO] –°—Ç–∞—Ç—å–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
#     print("[INFO] –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω!")



def fetch_html(url, retries=5, backoff=5):
    """
    –ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–∫–∏ 429 (Too Many Requests)
    –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π.

    :param url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    :param retries: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
    :param backoff: –Ω–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    :return: —Ç–µ–∫—Å—Ç HTML –∏–ª–∏ None –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    }

    session = requests.Session()
    session.headers.update(headers)

    for attempt in range(1, retries + 1):
        try:
            response = session.get(url, timeout=10)
            response.raise_for_status()
            return response.text

        except requests.exceptions.HTTPError as e:
            if response.status_code == 429:
                print(f"[WARN] 429 Too Many Requests for {url}. "
                      f"Attempt {attempt}/{retries}. Waiting {backoff} sec...")
                time.sleep(backoff)
                backoff *= 2  # —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            else:
                print(f"[ERROR] HTTP error for {url}: {e}")
                return None

        except requests.exceptions.RequestException as e:
            print(f"[ERROR] Failed to fetch {url}: {e}")
            return None

    print(f"[ERROR] Max retries exceeded for {url}")
    return None


# def parse_article(url):
#     html = fetch_html(url)
#     if not html:
#         return None
#
#     soup = BeautifulSoup(html, "html.parser")
#
#     # –ó–∞–≥–æ–ª–æ–≤–æ–∫
#     title_tag = soup.find("h1")
#     title = title_tag.get_text(strip=True) if title_tag else "No Title"
#
#     # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
#     date_tag = soup.find("meta", {"property": "article:published_time"})
#     if date_tag and date_tag.get("content"):
#         try:
#             date = datetime.fromisoformat(date_tag.get("content").replace("Z", "+00:00")).isoformat()
#         except:
#             date = None
#     else:
#         date = None
#
#     # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
#     article_body = soup.find("div", class_="ArticleBody__content___2gQno")
#     if article_body:
#         paragraphs = article_body.find_all("p")
#     else:
#         paragraphs = soup.find_all("p")  # fallback –Ω–∞ –≤—Å–µ <p>
#
#     text = "\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
#
#     if not text:
#         return None
#
#     return {
#         "title": title,
#         "text": text,
#         "date": date,
#         "source": "AP News",
#         "label": "real"
#     }

def parse_article(url, output_file):
    html = fetch_html(url)
    if not html:
        return None

    soup = BeautifulSoup(html, "html.parser")

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else "No Title"

    # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
    date_tag = soup.find("meta", {"property": "article:published_time"})
    if date_tag and date_tag.get("content"):
        try:
            date = datetime.fromisoformat(
                date_tag.get("content").replace("Z", "+00:00")
            ).isoformat()
        except:
            date = None
    else:
        date = None

    # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
    article_body = soup.find("div", class_="ArticleBody__content___2gQno")
    if article_body:
        paragraphs = article_body.find_all("p")
    else:
        paragraphs = soup.find_all("p")

    text = "\n".join(
        [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
    )

    if not text:
        return None

    article = {
        "title": title,
        "text": text,
        "date": date,
        "source": "AP News",
        "label": "real"
    }

    # üîπ –ú–ì–ù–û–í–ï–ù–ù–ê–Ø –∑–∞–ø–∏—Å—å –≤ JSONL
    with open(output_file, "a", encoding="utf-8") as f:
        f.write(json.dumps(article, ensure_ascii=False) + "\n")

    return article


def get_article_links(section_url, max_articles=100):
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ —Å —Ä–∞–∑–¥–µ–ª–∞ AP News (HTML)."""
    collected_links = set()
    html = fetch_html(section_url)
    if not html:
        return []

    soup = BeautifulSoup(html, "html.parser")

    # –ù–∞—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫–∏ –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
    for a in soup.find_all("a", href=True):
        href = a['href']
        # AP News —Å—Ç–∞—Ç—å–∏ –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç apnews.com/article/
        if "apnews.com/article/" in href:
            full_url = href if href.startswith("http") else "https://apnews.com" + href
            collected_links.add(full_url)
        if len(collected_links) >= max_articles:
            break

    print(f"[INFO] Collected {len(collected_links)} article links")
    return list(collected_links)[:max_articles]